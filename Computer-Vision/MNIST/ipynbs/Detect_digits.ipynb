{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_MODEL_PATH = \"Models\\CNN_MNIST\"\n",
    " \n",
    "def load_recognizer(model_path):\n",
    "    return  tf.keras.models.load_model(model_path)\n",
    "   \n",
    "   \n",
    "def preprocess_for_recognize(img):\n",
    "    \n",
    "    img = cv2.resize(img,(28,28))\n",
    "    \n",
    "    img = img / 255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "recognizer = load_recognizer(CNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/non-maxima-suppression-139f7e00f0b5\n",
    "def NMS(boxes, overlapThresh = 0.4):\n",
    "    #return an empty list, if no boxes given\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    x1 = boxes[:, 0]  # x coordinate of the top-left corner\n",
    "    y1 = boxes[:, 1]  # y coordinate of the top-left corner\n",
    "    x2 = boxes[:, 2]  # x coordinate of the bottom-right corner\n",
    "    y2 = boxes[:, 3]  # y coordinate of the bottom-right corner\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1) # We have a least a box of one pixel, therefore the +1\n",
    "    indices = np.arange(len(x1))\n",
    "    for i,box in enumerate(boxes):\n",
    "        temp_indices = indices[indices!=i]\n",
    "        xx1 = np.maximum(box[0], boxes[temp_indices,0])\n",
    "        yy1 = np.maximum(box[1], boxes[temp_indices,1])\n",
    "        xx2 = np.minimum(box[2], boxes[temp_indices,2])\n",
    "        yy2 = np.minimum(box[3], boxes[temp_indices,3])\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / areas[temp_indices]\n",
    "        if np.any(overlap) > overlapThresh:\n",
    "            indices = indices[indices != i]\n",
    "    selected_boxes = boxes[indices].astype(int)\n",
    "\n",
    "    # add 10px margin on bounding box\n",
    "    selected_boxes[:,0] = selected_boxes[:,0] - 10\n",
    "    selected_boxes[:,1] = selected_boxes[:,1] - 10\n",
    "    selected_boxes[:,2] = selected_boxes[:,2] + 10\n",
    "    selected_boxes[:,3] = selected_boxes[:,3] + 10\n",
    "\n",
    "    return selected_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_digits(img_path):\n",
    "    # https://stackoverflow.com/a/57623749\n",
    "    mser = cv2.MSER_create()\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    regions, boundingBoxes = mser.detectRegions(gray)\n",
    "\n",
    "    boundingBoxes = np.array(boundingBoxes)\n",
    "    \n",
    "    # convert x,y,w,h -> x1,y1,x2,y2\n",
    "    boundingBoxes[:,2] = boundingBoxes[:,2] + boundingBoxes[:,0]\n",
    "    boundingBoxes[:,3] = boundingBoxes[:,3] + boundingBoxes[:,1]\n",
    "    \n",
    "    return NMS(boundingBoxes)\n",
    "    \n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_digits(image_path):\n",
    "    \n",
    "    digits = []\n",
    "    \n",
    "    im_gray = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "    (thresh, im_bw) = cv2.threshold(im_gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    thresh = 127\n",
    "    im_bw = cv2.threshold(im_gray, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # convert text to white color \n",
    "    # im_bw = cv2.bitwise_not(im_bw)\n",
    "       \n",
    "    selected_bounding_boxes = detect_digits(image_path)\n",
    "    \n",
    "    processed_images = []\n",
    "    for bounding_box in selected_bounding_boxes:\n",
    "        \n",
    "        x1,y1, x2,y2 = bounding_box\n",
    "        \n",
    "        crop_img = im_bw[y1:y2,x1:x2]\n",
    "        \n",
    "        processed_img = preprocess_for_recognize(crop_img)\n",
    "        \n",
    "        processed_images.append(processed_img)\n",
    "        \n",
    "    processed_images = np.array(processed_images)\n",
    "    \n",
    "    processed_images = np.expand_dims(processed_images,axis=-1)    \n",
    "        \n",
    "    prediction = recognizer(processed_images)\n",
    "\n",
    "    digits = np.argmax(prediction,axis=1)\n",
    "    \n",
    "    return digits\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognize_digits(\"four.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a01596df69f18fb45dfbde46d90c3aa2bca9846212fe9428edbe4df54fd4e230"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
