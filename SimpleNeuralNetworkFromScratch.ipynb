{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleFeedForwardNeuralNetwork",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u44fp4C2v6oW",
        "colab_type": "text"
      },
      "source": [
        "# Single Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8km5y8xxEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEP6OuvHv1Np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron():\n",
        "  \"\"\" \n",
        "  Simple feed forward neuron\n",
        "\n",
        "  Args\n",
        "    num_input (Int) : number of input value\n",
        "    activation_fn (callable) : activation function\n",
        "  Attributes:\n",
        "    W (ndarray) : weights value for each input\n",
        "    b (float) : bias\n",
        "    activation (callable) :activation function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_inputs,activation_fn):\n",
        "    \"\"\" intialize weight and bias randomly\"\"\"\n",
        "    self.W = np.random.rand(num_inputs)\n",
        "    self.b = np.random.rand(1)\n",
        "    self.activation = activation_fn\n",
        "\n",
        "  def forward(self,X):\n",
        "    z = np.dot(X,self.W) + self.b # matrix multiplication and add bias(z=x.W+b)\n",
        "    result = self.activation(z) # apply activation function\n",
        "    return result\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERSQSCoDz4fu",
        "colab_type": "code",
        "outputId": "46bb1534-025e-4154-8264-6251a10ad2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = np.random.rand(3).reshape(1, 3) # random array (1X3)\n",
        "\n",
        "activation_function = lambda y: 0 if y <= 0 else 1 #if y negative or 0 return 0 else 1\n",
        "\n",
        "perceptron = Neuron(num_inputs=x.size, activation_fn=activation_function) #create perceptron\n",
        "\n",
        "perceptron.forward(x) # apply forward function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqkSryotwrrB",
        "colab_type": "text"
      },
      "source": [
        "# Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ud8P78wqnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU8DyIyG0MkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedLayer():\n",
        "  \"\"\"\n",
        "  Args\n",
        "  num_input (int) : number of input\n",
        "  layer_size (int) : layer size\n",
        "  activation_fn (callable) : activation function\n",
        "\n",
        "  Attributes\n",
        "  W (ndarray) : matrix of weights (num_input X layer_size)\n",
        "  b (ndarray) : array of bias\n",
        "  activation_fn (callable): activation function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_input,layer_size,activation_fn):\n",
        "    \"\"\"\n",
        "    initalize weight and bias\n",
        "    \"\"\"\n",
        "    self.W = np.random.standard_normal(size=(num_input,layer_size))\n",
        "    self.b = np.random.standard_normal(size=layer_size)\n",
        "    self.activation_fn = activation_fn\n",
        "\n",
        "  def forward(self,X):\n",
        "    z = np.dot(X,self.W) + self.b\n",
        "    result = self.activation_fn(z)\n",
        "    return result\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Ul41JH_Yh8",
        "colab_type": "code",
        "outputId": "9d009afb-ce28-4958-f7e0-e79e1ca7c68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "input_size = 2 (x1,x2)\n",
        "layer_size = 5\n",
        "outpur_size = 5\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "x = np.random.rand(2).reshape(1,2) # random array (1 X 2)\n",
        "\n",
        "activation_fn = lambda z : np.maximum(z,0) #relu\n",
        "\n",
        "layer = FullyConnectedLayer(x.size,5,activation_fn) # intialize layer\n",
        "\n",
        "print(layer.forward(x)) # apply forward function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         1.63021409 1.35667062 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Or4X9jHScw",
        "colab_type": "code",
        "outputId": "1282e872-361e-4440-de6f-bd9b6c158cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# in above cell we see example with one input array\n",
        "# let apply more than one input array on connected layer\n",
        "\n",
        "x1 = np.random.rand(2).reshape(1,2) # random array (1 X 2)\n",
        "x2 = np.random.rand(2).reshape(1,2) # random array (1 X 2)\n",
        "\n",
        "batch = np.concatenate((x1,x2)) # stcak two array\n",
        "layer.forward(batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0625052 , 0.73118069, 0.19824003, 0.        , 0.        ],\n",
              "       [0.        , 1.1124988 , 1.19359605, 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kzOK-ZfxaJc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> we get  matrix 2X5 which means 5 values for each input as we use 2 input array in input\n",
        "\n",
        "\n",
        "> This result of concatenation of input array called batch\n",
        "\n",
        "\n",
        "> In our example batch size is as we use input array at a time to forward operation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW9OXgo70ro7",
        "colab_type": "text"
      },
      "source": [
        "# Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAFR5gPGrk3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedLayer():\n",
        "  \"\"\"\n",
        "  Args\n",
        "  num_input (int) : number of input\n",
        "  layer_size (int) : layer size\n",
        "  activation_fn (callable) : activation function\n",
        "\n",
        "  Attributes\n",
        "  W (ndarray) : matrix of weights (num_input X layer_size)\n",
        "  b (ndarray) : array of bias\n",
        "  activation_fn (callable): activation function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_input,layer_size,activation_fn):\n",
        "    \"\"\"\n",
        "    initalize weight and bias\n",
        "    \"\"\"\n",
        "    self.W = np.random.standard_normal(size=(num_input,layer_size))\n",
        "    self.b = np.random.standard_normal(size=layer_size)\n",
        "    self.activation_fn = activation_fn\n",
        "\n",
        "  def forward(self,X):\n",
        "    z = np.dot(X,self.W) + self.b\n",
        "    result = self.activation_fn(z)\n",
        "    return result\n",
        "\n",
        "\n",
        "class FeedForwardNN():\n",
        "  \"\"\"\n",
        "  simple feed forward neural network\n",
        "\n",
        "  Args\n",
        "  num_input (int) : number of input values\n",
        "  num_output (int) : number of output neuron\n",
        "  hidden_layer_size (list) : list of size of each hidden layer\n",
        "  activation_fn (callable) : activation function\n",
        "\n",
        "  Attributes\n",
        "  layers (list) : list of layers which will make neural network\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_input,num_output,hidden_layer_size,activation_fn):\n",
        "    \"\"\"\n",
        "    for simplycity we will use same activation in each layer\n",
        "    \"\"\"\n",
        "    sizes = [num_input,*hidden_layer_size,num_output]\n",
        "    self.layers = [FullyConnectedLayer(sizes[i],sizes[i+1],activation_fn)\\\n",
        "                   for i in range(len(sizes)-1)]\n",
        "      \n",
        "  def forward(self,X):\n",
        "    \"\"\"\n",
        "    perform forward operation on each layer\n",
        "    \"\"\"\n",
        "    for layer in self.layers:\n",
        "      X = layer.forward(X)\n",
        "    return X\n",
        "\n",
        "  def predict(self,X):\n",
        "    \"\"\"\n",
        "    call forward method and apply argmax on result to get index of result\n",
        "    \"\"\"\n",
        "    estimation = self.forward(X)\n",
        "    result = np.argmax(estimation)\n",
        "    return result\n",
        "\n",
        "  def evaluate_score(self,X_test,y_test):\n",
        "    \"\"\"\n",
        "    evaluate accuracy on test set\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(X_test)):\n",
        "      if self.predict(X_test[i]) == y_test[i]:\n",
        "        correct_predictions += 1\n",
        "    return correct_predictions / len(X_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrRovWMqNa2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#activation fun\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NqY75COKvMR",
        "colab_type": "code",
        "outputId": "c607b9a7-e60f-47e3-a5f2-0c606c16e564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install mnist"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.18.4)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPSTj77q_z-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use mnist dataset for this feed forward networ\n",
        "import numpy as np\n",
        "import mnist\n",
        "\n",
        "# Loading the training and testing data:\n",
        "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
        "X_test, y_test = mnist.test_images(), mnist.test_labels()\n",
        "\n",
        "num_classes = 10\n",
        "# classes are the digits from 0 to 9\n",
        "\n",
        "# We transform the images into column vectors (as inputs for our NN):\n",
        "X_train, X_test = X_train.reshape(-1, 28*28), X_test.reshape(-1, 28*28)  #(748,)\n",
        "\n",
        "# We \"one-hot\" the labels\n",
        "y_train_one_hot = np.zeros((y_train.size, y_train.max()+1))\n",
        "y_train_one_hot[np.arange(y_train.size),y_train] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKxOqkcgMPK1",
        "colab_type": "code",
        "outputId": "0eb09dce-6f7f-4331-ec14-b3adfec3b362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_one_hot[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuEdZ17aMYLq",
        "colab_type": "code",
        "outputId": "f6ce2787-ec9a-4a8c-de49-284a9f212476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_3HXmwkMbEn",
        "colab_type": "code",
        "outputId": "46217b28-4dbf-4677-f167-96903b4d85d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# here we are using feedforward network so there is no training we jusd pass input and get output\n",
        "\n",
        "mnist_classifier = FeedForwardNN(X_train.shape[1],num_classes,[64,32],sigmoid)\n",
        "accuracy = mnist_classifier.evaluate_score(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxnjBu9gMl5O",
        "colab_type": "code",
        "outputId": "22eeb992-9332-4193-d21f-40a76438c124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.148333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_suxLNpVTV9h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Here we get only 10.14% accuracy as we define neural network with random parameters.\n",
        "\n",
        "\n",
        "> for better accuracy we need to train neural network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xp7JjaZlFc-",
        "colab_type": "text"
      },
      "source": [
        "# Simple Neural Network With Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwO8i5FNSqN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedLayer():\n",
        "  \"\"\"\n",
        "  Args\n",
        "  num_input (int) : number of input\n",
        "  layer_size (int) : layer size\n",
        "  activation_fn (callable) : activation function\n",
        "  d_activation_fn (callable) : differentiation of activation function\n",
        "\n",
        "  Attributes\n",
        "  W (ndarray) : matrix of weights (num_input X layer_size)\n",
        "  b (ndarray) : array of bias\n",
        "  activation_fn (callable): activation function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_input,layer_size,activation_fn,d_activation_fn):\n",
        "    \"\"\"\n",
        "    initalize weight and bias\n",
        "    \"\"\"\n",
        "    self.W = np.random.standard_normal(size=(num_input,layer_size))\n",
        "    self.b = np.random.standard_normal(size=layer_size)\n",
        "    self.activation_fn = activation_fn\n",
        "    self.d_activation_fn = d_activation_fn\n",
        "  \n",
        "  def forward(self,X):\n",
        "    self.z = np.dot(X,self.W) + self.b\n",
        "    self.y_hat = self.activation_fn(self.z)\n",
        "    self.X = X # we store X  for backropagation\n",
        "    return self.y_hat\n",
        "\n",
        "  def backward(self,dL_dy):\n",
        "    # dL_dy : derivation of loss function w.r.t y\n",
        "\n",
        "    # derivation of activation function w.r.t z\n",
        "    dy_dz = self.d_activation_fn(self.y_hat)\n",
        "\n",
        "    # derivation of Loss function w.r.t to z (chain rule) dL/dz = dL/dy * dy/dz\n",
        "    dL_dz = dL_dy * dy_dz \n",
        "\n",
        "    # derivation of z with respect to weight dz/dw = x  (x.w/w = x)\n",
        "    dz_dw = self.X.T\n",
        "\n",
        "    # derivation of z with respect to X dz/dx = w\n",
        "    dz_dx = self.W.T\n",
        "\n",
        "    # derivation of loss with respect to weight dL_d_w = dL_dy*dy_dz*dz_dw = dz_dw*dL_dz\n",
        "    self.dL_dw = np.dot(dz_dw, dL_dz) #storing for updating weights\n",
        "\n",
        "    # derivation of z w.r.t b  dz_db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
        "    dz_db = np.ones(dL_dy.shape[0]) #storing for updating bias\n",
        "    self.dL_db = np.dot(dz_db, dL_dz)\n",
        "\n",
        "    # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
        "    dL_dx = np.dot(dL_dz, dz_dx)\n",
        "\n",
        "    return dL_dx\n",
        "\n",
        "  def optimize(self, learning_rate):\n",
        "    \"\"\"\n",
        "    Optimize the layer's parameters w.r.t. the derivative values.\n",
        "    \"\"\"\n",
        "    self.W -= learning_rate * self.dL_dw #update weights\n",
        "    self.b -= learning_rate * self.dL_db #update bias\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV8psw670Zra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sigmoid function\n",
        "def sigmoid(x):     \n",
        "    return 1 / (1 + np.exp(-x)) # y\n",
        "\n",
        "# sigmoid derivative function\n",
        "def derivated_sigmoid(y):   \n",
        "    return y * (1 - y)\n",
        "\n",
        "# L2 loss function\n",
        "def loss_L2(pred, target):    \n",
        "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
        "\n",
        "# L2 derivative function\n",
        "def derivated_loss_L2(pred, target):    \n",
        "    return 2 * (pred - target)\n",
        "\n",
        "# cross-entropy loss function\n",
        "def cross_entropy(pred, target):    \n",
        "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
        "\n",
        "# cross-entropy derivative function\n",
        "def derivated_cross_entropy(pred, target):    \n",
        "    return (pred - target) / (pred * (1 - pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhziuEW31Ank",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleNeuralNetwork():\n",
        "  \"\"\"\n",
        "  Args:\n",
        "\n",
        "    num_input (int) : number of input values\n",
        "    num_output (int) : number of output neuron\n",
        "    hidden_layer_size (list) : list of size of each layer\n",
        "    activation_fn (callable) : activation function\n",
        "    d_activation_fn (callable) : derivative of activation function\n",
        "    loss_fn (callable) : loss function to train this network\n",
        "    d_loss_fn (callable) : The derivative of the loss function, for back-propagation\n",
        "\n",
        "  Attributes:\n",
        "    layers (list): list of size of each layer.\n",
        "    loss_fn (callable): loss function to train this network.\n",
        "    d_loss_fn (callable): The derivative of the loss function, for back-propagation.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,num_input,num_output,hidden_layer_size = [64,32],\\\n",
        "               activation_fn = sigmoid,d_activation_fn = derivated_sigmoid,\\\n",
        "               loss_fn=loss_L2,d_loss_fn=derivated_loss_L2):\n",
        "    layer_sizes = [num_input, *hidden_layer_size, num_output]\n",
        "    self.layers = [\\\n",
        "                   FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], activation_fn, d_activation_fn)\\\n",
        "                   for i in range(len(layer_sizes) - 1)]\n",
        "    self.activation_fn = activation_fn\n",
        "    self.loss_fn = loss_fn\n",
        "    self.d_loss_fn = d_loss_fn\n",
        "  \n",
        "  def forward(self,X):\n",
        "    for layer in self.layers:\n",
        "      X = layer.forward(X)\n",
        "    return X\n",
        "  \n",
        "  def predict(self,X):\n",
        "    z = self.forward(X)\n",
        "    y_hat = np.argmax(self.activation_fn(z))\n",
        "    return y_hat\n",
        "\n",
        "  def backward(self,dL_dy):\n",
        "    \"\"\"\n",
        "    backward operation Back propagation\n",
        "    \"\"\"\n",
        "    for layer in reversed(self.layers):\n",
        "      dL_dy = layer.backward(dL_dy)\n",
        "    return dL_dy\n",
        "\n",
        "  def optimize(self,learning_rate):\n",
        "    for layer in self.layers:\n",
        "      layer.optimize(learning_rate)\n",
        "\n",
        "  def evaluate_score(self,X_test,y_test):\n",
        "    \"\"\"\n",
        "    evaluate accuracy on test set\n",
        "    \"\"\"\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(X_test)):\n",
        "      if self.predict(X_test[i]) == y_test[i]:\n",
        "        correct_predictions += 1\n",
        "    return correct_predictions / len(X_test)\n",
        "\n",
        "  def train(self, X_train, y_train, X_val=None, y_val=None, batch_size=32, num_epochs=5, learning_rate=1e-3):\n",
        "    num_batches_per_epoch = len(X_train) // batch_size\n",
        "    do_validation = X_val is not None and y_val is not None\n",
        "    losses, accuracies = [], []\n",
        "    for epoch in range(num_epochs):\n",
        "      epoch_loss = 0\n",
        "      for b in range(num_batches_per_epoch):\n",
        "        #create batch\n",
        "        b_idx = b * batch_size\n",
        "        b_idx_e = b_idx + batch_size\n",
        "        x, y_true = X_train[b_idx:b_idx_e], y_train[b_idx:b_idx_e]\n",
        "\n",
        "        # Optimize on batch:\n",
        "        y = self.forward(x) # forward pass\n",
        "        epoch_loss += self.loss_fn(y, y_true) # loss\n",
        "        dL_dy = self.d_loss_fn(y, y_true) # loss derivation\n",
        "        self.backward(dL_dy) # back-propagation pass\n",
        "        self.optimize(learning_rate) # optimization\n",
        "\n",
        "      losses.append(epoch_loss / num_batches_per_epoch)\n",
        "      # After each epoch, we \"validate\" our network, i.e., we measure its accuracy over the test/validation set:\n",
        "      accuracies.append(self.evaluate_score(X_val, y_val))\n",
        "      print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(epoch, losses[epoch], accuracies[epoch] * 100))\n",
        "    return losses, accuracies\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EHqx5v6G3TM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Loading the training and testing data:\n",
        "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
        "X_test, y_test = mnist.test_images(), mnist.test_labels()\n",
        "\n",
        "num_classes = 10\n",
        "# classes are the digits from 0 to 9\n",
        "\n",
        "# We transform the images into column vectors (as inputs for our NN):\n",
        "X_train, X_test = X_train.reshape(-1, 28*28), X_test.reshape(-1, 28*28)  #(748,)\n",
        "\n",
        "# We \"one-hot\" the labels\n",
        "y_train_one_hot = np.zeros((y_train.size, y_train.max()+1))\n",
        "y_train_one_hot[np.arange(y_train.size),y_train] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSKZd7_JBkrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a755e1c4-94f9-48aa-8918-208fbb2a8bbf"
      },
      "source": [
        "num_classes = 10\n",
        "mnist_classifier = SimpleNeuralNetwork(X_train.shape[1],num_classes)\n",
        "losses, accuracies = mnist_classifier.train(X_train, y_train_one_hot, X_test, y_test, batch_size=32, num_epochs=100)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch    0: training loss = 1.003200 | val accuracy = 26.59%\n",
            "Epoch    1: training loss = 0.822031 | val accuracy = 37.87%\n",
            "Epoch    2: training loss = 0.736707 | val accuracy = 45.53%\n",
            "Epoch    3: training loss = 0.669756 | val accuracy = 51.74%\n",
            "Epoch    4: training loss = 0.614017 | val accuracy = 57.32%\n",
            "Epoch    5: training loss = 0.564004 | val accuracy = 61.85%\n",
            "Epoch    6: training loss = 0.524683 | val accuracy = 65.27%\n",
            "Epoch    7: training loss = 0.489468 | val accuracy = 67.81%\n",
            "Epoch    8: training loss = 0.463183 | val accuracy = 70.21%\n",
            "Epoch    9: training loss = 0.439475 | val accuracy = 72.22%\n",
            "Epoch   10: training loss = 0.419056 | val accuracy = 73.56%\n",
            "Epoch   11: training loss = 0.402877 | val accuracy = 75.21%\n",
            "Epoch   12: training loss = 0.386778 | val accuracy = 76.34%\n",
            "Epoch   13: training loss = 0.374423 | val accuracy = 76.90%\n",
            "Epoch   14: training loss = 0.360682 | val accuracy = 78.03%\n",
            "Epoch   15: training loss = 0.353163 | val accuracy = 78.49%\n",
            "Epoch   16: training loss = 0.344533 | val accuracy = 79.15%\n",
            "Epoch   17: training loss = 0.332130 | val accuracy = 79.92%\n",
            "Epoch   18: training loss = 0.323844 | val accuracy = 79.99%\n",
            "Epoch   19: training loss = 0.315640 | val accuracy = 81.01%\n",
            "Epoch   20: training loss = 0.309109 | val accuracy = 81.30%\n",
            "Epoch   21: training loss = 0.302776 | val accuracy = 81.68%\n",
            "Epoch   22: training loss = 0.296234 | val accuracy = 81.67%\n",
            "Epoch   23: training loss = 0.288958 | val accuracy = 82.57%\n",
            "Epoch   24: training loss = 0.282867 | val accuracy = 82.59%\n",
            "Epoch   25: training loss = 0.276176 | val accuracy = 82.71%\n",
            "Epoch   26: training loss = 0.275286 | val accuracy = 83.23%\n",
            "Epoch   27: training loss = 0.271157 | val accuracy = 83.64%\n",
            "Epoch   28: training loss = 0.264987 | val accuracy = 83.89%\n",
            "Epoch   29: training loss = 0.264351 | val accuracy = 83.53%\n",
            "Epoch   30: training loss = 0.256860 | val accuracy = 84.35%\n",
            "Epoch   31: training loss = 0.247523 | val accuracy = 84.96%\n",
            "Epoch   32: training loss = 0.247088 | val accuracy = 85.09%\n",
            "Epoch   33: training loss = 0.245692 | val accuracy = 85.19%\n",
            "Epoch   34: training loss = 0.240365 | val accuracy = 85.17%\n",
            "Epoch   35: training loss = 0.237457 | val accuracy = 85.15%\n",
            "Epoch   36: training loss = 0.237091 | val accuracy = 85.41%\n",
            "Epoch   37: training loss = 0.233690 | val accuracy = 85.36%\n",
            "Epoch   38: training loss = 0.230236 | val accuracy = 85.51%\n",
            "Epoch   39: training loss = 0.226852 | val accuracy = 86.43%\n",
            "Epoch   40: training loss = 0.223214 | val accuracy = 86.41%\n",
            "Epoch   41: training loss = 0.221077 | val accuracy = 86.48%\n",
            "Epoch   42: training loss = 0.218193 | val accuracy = 86.93%\n",
            "Epoch   43: training loss = 0.215326 | val accuracy = 86.90%\n",
            "Epoch   44: training loss = 0.212584 | val accuracy = 87.07%\n",
            "Epoch   45: training loss = 0.211297 | val accuracy = 87.38%\n",
            "Epoch   46: training loss = 0.208865 | val accuracy = 87.05%\n",
            "Epoch   47: training loss = 0.215087 | val accuracy = 87.09%\n",
            "Epoch   48: training loss = 0.208524 | val accuracy = 87.58%\n",
            "Epoch   49: training loss = 0.204888 | val accuracy = 87.36%\n",
            "Epoch   50: training loss = 0.206094 | val accuracy = 87.25%\n",
            "Epoch   51: training loss = 0.204526 | val accuracy = 87.50%\n",
            "Epoch   52: training loss = 0.200076 | val accuracy = 87.73%\n",
            "Epoch   53: training loss = 0.197416 | val accuracy = 87.69%\n",
            "Epoch   54: training loss = 0.197742 | val accuracy = 87.43%\n",
            "Epoch   55: training loss = 0.197465 | val accuracy = 87.99%\n",
            "Epoch   56: training loss = 0.194322 | val accuracy = 87.78%\n",
            "Epoch   57: training loss = 0.190362 | val accuracy = 88.17%\n",
            "Epoch   58: training loss = 0.189252 | val accuracy = 88.09%\n",
            "Epoch   59: training loss = 0.189243 | val accuracy = 88.42%\n",
            "Epoch   60: training loss = 0.186447 | val accuracy = 88.74%\n",
            "Epoch   61: training loss = 0.186148 | val accuracy = 88.71%\n",
            "Epoch   62: training loss = 0.184777 | val accuracy = 88.49%\n",
            "Epoch   63: training loss = 0.183228 | val accuracy = 88.82%\n",
            "Epoch   64: training loss = 0.183200 | val accuracy = 88.66%\n",
            "Epoch   65: training loss = 0.185780 | val accuracy = 88.75%\n",
            "Epoch   66: training loss = 0.182517 | val accuracy = 88.92%\n",
            "Epoch   67: training loss = 0.179981 | val accuracy = 89.33%\n",
            "Epoch   68: training loss = 0.179456 | val accuracy = 89.00%\n",
            "Epoch   69: training loss = 0.177300 | val accuracy = 89.14%\n",
            "Epoch   70: training loss = 0.175246 | val accuracy = 88.73%\n",
            "Epoch   71: training loss = 0.177239 | val accuracy = 89.18%\n",
            "Epoch   72: training loss = 0.175373 | val accuracy = 89.15%\n",
            "Epoch   73: training loss = 0.173229 | val accuracy = 89.36%\n",
            "Epoch   74: training loss = 0.168322 | val accuracy = 89.65%\n",
            "Epoch   75: training loss = 0.168825 | val accuracy = 89.52%\n",
            "Epoch   76: training loss = 0.171156 | val accuracy = 89.24%\n",
            "Epoch   77: training loss = 0.170465 | val accuracy = 89.15%\n",
            "Epoch   78: training loss = 0.171984 | val accuracy = 89.31%\n",
            "Epoch   79: training loss = 0.168861 | val accuracy = 89.71%\n",
            "Epoch   80: training loss = 0.168217 | val accuracy = 89.96%\n",
            "Epoch   81: training loss = 0.171336 | val accuracy = 89.52%\n",
            "Epoch   82: training loss = 0.173784 | val accuracy = 89.42%\n",
            "Epoch   83: training loss = 0.171008 | val accuracy = 89.41%\n",
            "Epoch   84: training loss = 0.171238 | val accuracy = 89.69%\n",
            "Epoch   85: training loss = 0.168460 | val accuracy = 89.54%\n",
            "Epoch   86: training loss = 0.167264 | val accuracy = 89.79%\n",
            "Epoch   87: training loss = 0.166012 | val accuracy = 89.87%\n",
            "Epoch   88: training loss = 0.166208 | val accuracy = 89.69%\n",
            "Epoch   89: training loss = 0.162677 | val accuracy = 90.29%\n",
            "Epoch   90: training loss = 0.162349 | val accuracy = 89.77%\n",
            "Epoch   91: training loss = 0.163651 | val accuracy = 89.91%\n",
            "Epoch   92: training loss = 0.163156 | val accuracy = 89.69%\n",
            "Epoch   93: training loss = 0.164857 | val accuracy = 89.80%\n",
            "Epoch   94: training loss = 0.164720 | val accuracy = 89.69%\n",
            "Epoch   95: training loss = 0.164873 | val accuracy = 89.84%\n",
            "Epoch   96: training loss = 0.162888 | val accuracy = 89.80%\n",
            "Epoch   97: training loss = 0.160705 | val accuracy = 89.70%\n",
            "Epoch   98: training loss = 0.158534 | val accuracy = 89.91%\n",
            "Epoch   99: training loss = 0.157056 | val accuracy = 90.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISAXvIbATsEr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> Here we can see by implementing back propagation we get around 90% in 100 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFQ_lvfaQqmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "+"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}