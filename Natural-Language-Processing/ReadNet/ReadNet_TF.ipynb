{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro-HTNjuCjAC"
      },
      "source": [
        "#Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "26l_5rGVClhF",
        "outputId": "beb38cf4-81a9-44fa-ff17-6be4d8cd4fe3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a367e9b-4191-4836-942a-bda4856d9ca0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a367e9b-4191-4836-942a-bda4856d9ca0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "api_token = files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlddlOtICugS",
        "outputId": "170118a8-6267-4207-e9fc-dd84a4266500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 38.1MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/6.79k [00:00<?, ?B/s]\n",
            "100% 6.79k/6.79k [00:00<00:00, 6.89MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/108 [00:00<?, ?B/s]\n",
            "100% 108/108 [00:00<00:00, 92.7kB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c commonlitreadabilityprize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etcu8qxyCuc6",
        "outputId": "d6ced4ca-09f1-4f12-e09f-9acebb3be6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/train.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTpNbU25Cuau",
        "outputId": "a61f12f0-7c82-44ee-e14a-9347fdbe0ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 13.6 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 52.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85446 sha256=26fde7288fdf84e45e47535cc8845f337770a96c2759a66a2ae6e50cd24c458b\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.2-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.11.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 47.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.11.0 textstat-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwTsqAPkCuYe",
        "outputId": "668aba2f-4f9a-4ccf-cc0e-b68ad4eba692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression,ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "from collections import Counter\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats import loguniform\n",
        "import csv\n",
        "from matplotlib import pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import contractions\n",
        "import textstat\n",
        "import bs4\n",
        "import requests\n",
        "import json\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import warnings\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "tqdm.pandas()\n",
        "pd.options.display.max_colwidth = None\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSkWA-7XCuWH"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dEOLRL-CuTY"
      },
      "outputs": [],
      "source": [
        "# we need only excerpt and target columns\n",
        "data = data[['excerpt','target']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIjU3beNQ081"
      },
      "outputs": [],
      "source": [
        "data['target'] = data['target'].astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2_Wd5tqQ6Vf",
        "outputId": "4c946234-208f-43a1-900b-2f4fdb5f94b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2834 entries, 0 to 2833\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   excerpt  2834 non-null   object \n",
            " 1   target   2834 non-null   float32\n",
            "dtypes: float32(1), object(1)\n",
            "memory usage: 33.3+ KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5tQPPbC8ea"
      },
      "outputs": [],
      "source": [
        "X = data['excerpt']\n",
        "y = data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhxWJVN7C8by"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BFgZqJtC8ZY"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.to_frame(name='excerpt')\n",
        "X_test = X_test.to_frame(name='excerpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyruBuQDC8XR"
      },
      "outputs": [],
      "source": [
        "# remove everythoing except a-z A-Z . and space\n",
        "regex_only_alpha_space_dot = re.compile('[^a-zA-Z. ]')\n",
        "\n",
        "# remove extra spaces\n",
        "regex_multiple_space = re.compile(\" +\")\n",
        "\n",
        "def pre_process_text(text):\n",
        "  text = contractions.fix(text)\n",
        "  text = regex_only_alpha_space_dot.sub(' ',text)\n",
        "  text = regex_multiple_space.sub(\" \",text)\n",
        "  return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnHXP2YTC8UV"
      },
      "outputs": [],
      "source": [
        "X_train['excerpt']= X_train['excerpt'].apply(pre_process_text)\n",
        "X_test['excerpt'] = X_test['excerpt'].apply(pre_process_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhe0VVd1DjNw"
      },
      "outputs": [],
      "source": [
        "doc_lens = []\n",
        "sent_lens =[]\n",
        "\n",
        "for text in X_train['excerpt'].values:\n",
        "  sentences = sent_tokenize(text)\n",
        "  doc_lens.append(len(sentences))\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    sent_lens.append(len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bqqaag8fEaUO",
        "outputId": "6f152f73-0b31-4897-b4ad-335b47ec7735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence length percentile :\n",
            "0 1.0\n",
            "5 6.0\n",
            "10 8.0\n",
            "15 9.0\n",
            "20 10.0\n",
            "25 12.0\n",
            "30 13.0\n",
            "35 14.0\n",
            "40 15.0\n",
            "45 17.0\n",
            "50 18.0\n",
            "55 19.0\n",
            "60 21.0\n",
            "65 23.0\n",
            "70 25.0\n",
            "75 27.0\n",
            "80 30.0\n",
            "85 33.0\n",
            "90 38.0\n",
            "95 46.0\n",
            "100 152.0\n"
          ]
        }
      ],
      "source": [
        "print(\"sentence length percentile :\")\n",
        "for i in range(0,105,5):\n",
        "  print(i,np.percentile(sent_lens,i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZrluaXpDjKf",
        "outputId": "4dedd265-8561-4871-f61c-feebbd3dc915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doc length percentile :\n",
            "0 2.0\n",
            "5 4.0\n",
            "10 5.0\n",
            "15 6.0\n",
            "20 6.0\n",
            "25 6.0\n",
            "30 7.0\n",
            "35 7.0\n",
            "40 7.0\n",
            "45 8.0\n",
            "50 8.0\n",
            "55 8.0\n",
            "60 9.0\n",
            "65 9.0\n",
            "70 10.0\n",
            "75 10.0\n",
            "80 11.0\n",
            "85 12.0\n",
            "90 14.0\n",
            "95 16.0\n",
            "100 34.0\n"
          ]
        }
      ],
      "source": [
        "print(\"doc length percentile :\")\n",
        "for i in range(0,105,5):\n",
        "  print(i,np.percentile(doc_lens,i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVVd8LGSDjIQ"
      },
      "outputs": [],
      "source": [
        "MAX_SENT_LEN = 46 #number of words\n",
        "MAX_DOC_LEN = 16 # number of sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTzA-_46DjF6"
      },
      "outputs": [],
      "source": [
        "def calculate_doc_features(doc):\n",
        "\n",
        "  def calculate_other_doc_features(doc):\n",
        "    words = word_tokenize(doc)\n",
        "    num_sents = len(sent_tokenize(doc))\n",
        "    num_words = len(words)\n",
        "\n",
        "\n",
        "    num_word_longer_than_6_char = 0\n",
        "    for word in words:\n",
        "      if len(word) > 6:\n",
        "        num_word_longer_than_6_char += 1\n",
        "\n",
        "    lix = (num_word_longer_than_6_char / num_words ) + (num_words / num_sents)\n",
        "    rix = num_word_longer_than_6_char / num_sents\n",
        "    lexical_diversity = len(set(words)) / num_words\n",
        "\n",
        "    return [lix,rix,lexical_diversity]\n",
        "\n",
        "  text_stat_features = [textstat.flesch_reading_ease(doc),\n",
        "                        textstat.flesch_kincaid_grade(doc),\n",
        "                        textstat.automated_readability_index(doc),\n",
        "                        textstat.coleman_liau_index(doc),\n",
        "                        textstat.gunning_fog(doc),\n",
        "                        textstat.smog_index(doc),\n",
        "                        textstat.dale_chall_readability_score(doc)\n",
        "                        ]\n",
        "  text_stat_features.extend(calculate_other_doc_features(doc))\n",
        "\n",
        "  return np.array(text_stat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcQ2uaVxDjDd"
      },
      "outputs": [],
      "source": [
        "def calculate_sent_features(doc,max_doc_len):\n",
        "\n",
        "  sentences = sent_tokenize(doc)\n",
        "  sent_features = []\n",
        "  for sent in sentences:\n",
        "\n",
        "    words = word_tokenize(sent)\n",
        "\n",
        "    num_words = len(words)\n",
        "\n",
        "    char_per_word = [len(word) for word in words]\n",
        "    avg_char_per_word = sum(char_per_word) / len(char_per_word)\n",
        "\n",
        "    num_long_words = len([word for word in words if len(word) > 6])\n",
        "\n",
        "    sent_features.append([num_words,avg_char_per_word,num_long_words])\n",
        "\n",
        "  sent_features = np.array(sent_features)\n",
        "  if len(sent_features) > max_doc_len:\n",
        "    sent_features = sent_features[:max_doc_len,:]\n",
        "  else:\n",
        "    pad_value = doc_pad_value = np.zeros((max_doc_len - sent_features.shape[0],\n",
        "                                          sent_features.shape[1]\n",
        "                                          )\n",
        "                                       )\n",
        "\n",
        "    sent_features = np.concatenate((sent_features,pad_value),axis=0)\n",
        "  return np.array(sent_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yqYZoHiDjBL",
        "outputId": "13a7b1ab-df10-4ff8-ebb5-16708296b68c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10,)\n",
            "(16, 3)\n"
          ]
        }
      ],
      "source": [
        "d = \"When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.\"\n",
        "\n",
        "print(calculate_doc_features(d).shape)\n",
        "print(calculate_sent_features(d,MAX_DOC_LEN).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uZ9pOpVFFX7"
      },
      "outputs": [],
      "source": [
        "X_train['sent_feat'] = X_train['excerpt'].apply(calculate_sent_features,args=(MAX_DOC_LEN,))\n",
        "X_test['sent_feat'] = X_test['excerpt'].apply(calculate_sent_features,args=(MAX_DOC_LEN,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOnp95YoFHSf"
      },
      "outputs": [],
      "source": [
        "X_train['doc_feat'] = X_train['excerpt'].apply(calculate_doc_features)\n",
        "X_test['doc_feat'] = X_test['excerpt'].apply(calculate_doc_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcMQjaqKCDq8"
      },
      "outputs": [],
      "source": [
        "class DocTokenizer:\n",
        "    def __init__(self,excerpts,max_doc_len,max_sent_len):\n",
        "\n",
        "        self.max_doc_len = max_doc_len\n",
        "        self.max_sent_len = max_sent_len\n",
        "\n",
        "        # will use tokenizer for \n",
        "        self.excerpt_tokenizer = Tokenizer(oov_token=\"oov_token\")\n",
        "        self.excerpt_tokenizer.fit_on_texts(excerpts)\n",
        "        self.vocab_size = len(self.excerpt_tokenizer.word_index) + 1\n",
        "         \n",
        "    def __call__(self, docs):\n",
        "        output = []\n",
        "\n",
        "        for doc in docs:\n",
        "          sentences = sent_tokenize(doc)\n",
        "          tokens = self.excerpt_tokenizer.texts_to_sequences(sentences)\n",
        "          \n",
        "          # 0 padded sentence\n",
        "          padded_tokens = pad_sequences(tokens,maxlen=self.max_sent_len,padding='post')\n",
        "\n",
        "          # 0 padded doc\n",
        "          # print(self.max_doc_len, padded_tokens.shape[0])\n",
        "          padded_doc = []\n",
        "          if self.max_doc_len > padded_tokens.shape[0]:\n",
        "            doc_pad_value = np.zeros(\n",
        "                (self.max_doc_len - padded_tokens.shape[0], self.max_sent_len)\n",
        "                )\n",
        "            padded_doc = np.concatenate((padded_tokens,doc_pad_value),axis=0)\n",
        "            \n",
        "          else:\n",
        "            padded_doc = padded_tokens[:self.max_doc_len,:]\n",
        "\n",
        "          output.append(padded_doc)\n",
        "\n",
        "        return np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3e3MniQFR02"
      },
      "outputs": [],
      "source": [
        "# https://towardsdatascience.com/implementing-custom-data-generators-in-keras-de56f013581c\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, tokens,sent_feat,doc_feat,y,batch_size=32,shuffle=True):\n",
        "        self.tokens = tokens\n",
        "        self.sent_feat = sent_feat\n",
        "        self.doc_feat = doc_feat\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = list(range(self.tokens.shape[0]))\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch = [self.indices[k] for k in index]\n",
        "        \n",
        "        batch_tokens, batch_sent_feat, batch_doc_feat, batch_y = self.__get_data(batch)\n",
        "        return [batch_tokens, batch_sent_feat, batch_doc_feat], batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.index = np.arange(len(self.indices))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.index)\n",
        "\n",
        "    def __get_data(self, batch):\n",
        "\n",
        "        batch_tokens = []\n",
        "        batch_sent_feat = []\n",
        "        batch_doc_feat = []\n",
        "        batch_y = []\n",
        "        for id in batch:\n",
        "            batch_tokens.append(self.tokens[id])\n",
        "            batch_sent_feat.append(self.sent_feat[id])\n",
        "            batch_doc_feat.append(self.doc_feat[id])\n",
        "            batch_y.append(self.y[id])\n",
        "\n",
        "        return [np.array(batch_tokens),\n",
        "                np.array(batch_sent_feat),\n",
        "                np.array(batch_doc_feat),\n",
        "                np.array(batch_y)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1cOuZY5FhR6"
      },
      "outputs": [],
      "source": [
        "tokenizer = DocTokenizer(X_train['excerpt'],MAX_DOC_LEN,MAX_SENT_LEN)\n",
        "X_train_tokens= tokenizer(X_train['excerpt'])\n",
        "X_test_tokens = tokenizer(X_test['excerpt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUriOQFXFiko",
        "outputId": "bf663e37-d8f6-47be-b38d-c452a3c3f4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train_tokens shape : (2267, 16, 46)\n",
            "X_test_tokens shape : (567, 16, 46)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train_tokens shape :\",X_train_tokens.shape)\n",
        "print(\"X_test_tokens shape :\",X_test_tokens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--KXqp4-FxGy"
      },
      "outputs": [],
      "source": [
        "X_train_sent_feat = np.array(X_train['sent_feat'].tolist())\n",
        "X_test_sent_feat = np.array(X_test['sent_feat'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZb-yKEwFyfR"
      },
      "outputs": [],
      "source": [
        "X_train_doc_feat = np.array(X_train['doc_feat'].tolist())\n",
        "X_test_doc_feat = np.array(X_test['doc_feat'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGlW92c2F0vI"
      },
      "outputs": [],
      "source": [
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2Hv-GQkFTWZ"
      },
      "outputs": [],
      "source": [
        "train_data_generator = DataGenerator(X_train_tokens,\n",
        "                                     X_train_sent_feat,\n",
        "                                     X_train_doc_feat,\n",
        "                                     y_train.values,\n",
        "                                     batch_size = batch_size)\n",
        "\n",
        "test_data_generator = DataGenerator(X_test_tokens,\n",
        "                                    X_test_sent_feat,\n",
        "                                    X_test_doc_feat,\n",
        "                                    y_test.values,\n",
        "                                    batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOsjKumCe-a"
      },
      "source": [
        "# ReadNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n51UEDBCcE3-"
      },
      "source": [
        "Ref:<br>\n",
        "https://www.tensorflow.org/text/tutorials/transformer <br>\n",
        "https://www.kaggle.com/renjithrrkj/readnet-implemetation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL-5MweVckRa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuFkYzDoZucw"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWUuc6nmcm7E"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvJfwlwHcobE"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1KrMHRhcseu"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "www9vhujcwNQ"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBAASURDcybR"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTRX9BNHd951"
      },
      "outputs": [],
      "source": [
        "class AttentionAggregation(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.query = tf.keras.layers.Dense(1,use_bias=False,activation='softmax')\n",
        "\n",
        "    def call(self, x):  # (b, s, m)\n",
        "        attns = self.query(x)\n",
        "        enc = tf.matmul(tf.transpose(attns,perm=[0,2,1]), x)  # (b, 1, m)\n",
        "\n",
        "        return tf.squeeze(enc,1)\n",
        "\n",
        "\n",
        "class LinTanh(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.lintanh = tf.keras.layers.Dense(d_model,use_bias=False,activation='tanh')\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.lintanh(x)\n",
        "\n",
        "\n",
        "class LinFeatConcat(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_out):\n",
        "        super().__init__()\n",
        "        self.lin = tf.keras.layers.Dense(n_out, use_bias=False)\n",
        "    def call(self, x, feats):\n",
        "        return self.lin(tf.concat([x, feats], axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG8wZzC2c0zL"
      },
      "outputs": [],
      "source": [
        "class SentenceEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(SentenceEncoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    self.lin_tanh = LinTanh(d_model=d_model)\n",
        "    self.attn_agg = AttentionAggregation()\n",
        "    self.lin_feat_concat = LinFeatConcat(n_out=self.d_model)\n",
        "\n",
        "  def call(self, inputs, training=False, mask=None):\n",
        "    x = inputs[0]\n",
        "    sent_feat = inputs[1]\n",
        "    seq_len = tf.shape(x)[2]\n",
        "\n",
        "    x = self.embedding(x)  #(batch_size x doc_len x sent_len x embed_size)\n",
        "\n",
        "    x_shape = tf.shape(x)\n",
        "    x = tf.reshape(x,(x_shape[0]*x_shape[1],\n",
        "                      x_shape[2],\n",
        "                      x_shape[3]\n",
        "                      )\n",
        "                  ) # (batch_size*doc_len, sent_len, d_model)\n",
        "\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    x = self.lin_tanh(x)\n",
        "    x = self.attn_agg(x)\n",
        "\n",
        "    \n",
        "    sent_feat = tf.reshape(sent_feat,(x_shape[0]*x_shape[1],-1))\n",
        "\n",
        "    x = self.lin_feat_concat(x, sent_feat) \n",
        "\n",
        "    return tf.reshape(x,(x_shape[0],x_shape[1],x_shape[-1])) # (batch_size,doc_len,embedding)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDBW5HNXdOw2"
      },
      "outputs": [],
      "source": [
        "class DocEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(DocEncoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
        "                                            d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
        "                       for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    self.lin_tanh = LinTanh(d_model=d_model)\n",
        "    self.attn_agg = AttentionAggregation()\n",
        "    self.lin_feat_concat = LinFeatConcat(n_out=self.d_model)\n",
        "\n",
        "  def call(self, inputs, training=False, mask=None):\n",
        "    x = inputs[0]\n",
        "    doc_feat = inputs[1]\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    x = self.lin_tanh(x)\n",
        "    x = self.attn_agg(x)\n",
        "\n",
        "    # sent_feat = tf.reshape(sent_feat,(x_shape[0]*x_shape[1],-1))\n",
        "    x = self.lin_feat_concat(x, doc_feat)\n",
        "\n",
        "    return x # (b,embedding)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-LTFf8hV7yF"
      },
      "outputs": [],
      "source": [
        "class ReadNet(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(ReadNet, self).__init__()\n",
        "\n",
        "    # get sentence features from words\n",
        "    self.S_en = SentenceEncoder(num_layers= num_layers,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dff=dff,\n",
        "                          input_vocab_size=input_vocab_size,\n",
        "                          maximum_position_encoding=maximum_position_encoding\n",
        "                          )\n",
        "\n",
        "    # get doc lever features from sentence features\n",
        "    self.D_en = DocEncoder(num_layers= num_layers,\n",
        "                     d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     input_vocab_size=input_vocab_size,\n",
        "                     maximum_position_encoding=maximum_position_encoding\n",
        "                     )\n",
        "\n",
        "    # output layer\n",
        "    self.final_layer = tf.keras.layers.Dense(1)\n",
        "  \n",
        "  def call(self,inputs,training=False):\n",
        "\n",
        "    x = inputs[0]\n",
        "    sent_features = inputs[1]\n",
        "    doc_features = inputs[2]\n",
        "\n",
        "    x = self.S_en([x,sent_features],training=training)\n",
        "\n",
        "    x = self.D_en([x,doc_features],training=training)\n",
        "\n",
        "    op = self.final_layer(x)\n",
        "\n",
        "    return op\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvbin8r1VW-k"
      },
      "outputs": [],
      "source": [
        "model = ReadNet(num_layers=6, d_model=512, num_heads=8,\n",
        "                         dff=1024, input_vocab_size=tokenizer.vocab_size,\n",
        "                         maximum_position_encoding=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q87WkVCWIPwy"
      },
      "outputs": [],
      "source": [
        "train_step_per_epoch = train_data_generator.__len__()\n",
        "test_step_per_epoch = test_data_generator.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUO6cHfyIsen"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = \"/content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint\"\n",
        "\n",
        "callback_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      patience=15,\n",
        "                                                      mode='auto',\n",
        "                                                      restore_best_weights=True\n",
        "                                                      )\n",
        "\n",
        "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_filepath,\n",
        "                                                         monitor='val_loss',\n",
        "                                                         save_best_only=True,\n",
        "                                                         save_weights_only=False,\n",
        "                                                         mode='auto',\n",
        "                                                         save_freq='epoch'\n",
        "                                                         )\n",
        "\n",
        "callback_reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                         factor=0.2,\n",
        "                                                         patience=5\n",
        "                                                         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Vq53k2Le8t"
      },
      "outputs": [],
      "source": [
        "# https://towardsdatascience.com/creating-custom-loss-functions-using-tensorflow-2-96c123d5ce6c\n",
        "def rmse_loss(y_true, y_pred):\n",
        "    #difference between true label and predicted label\n",
        "    error = y_true-y_pred    \n",
        "    #square of the error\n",
        "    sqr_error = K.square(error)\n",
        "    #mean of the square of the error\n",
        "    mean_sqr_error = K.mean(sqr_error)\n",
        "    #square root of the mean of the square of the error\n",
        "    sqrt_mean_sqr_error = K.sqrt(mean_sqr_error)\n",
        "    #return the error\n",
        "    return sqrt_mean_sqr_error\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=rmse_loss,\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMheb6-WHscL",
        "outputId": "24290765-fc11-4f70-b356-5613a406d700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "70/70 [==============================] - 147s 2s/step - loss: 22.4166 - val_loss: 15.5417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "70/70 [==============================] - 125s 2s/step - loss: 9.4762 - val_loss: 5.0232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100\n",
            "70/70 [==============================] - 125s 2s/step - loss: 7.4366 - val_loss: 6.8370\n",
            "Epoch 4/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 6.2708 - val_loss: 8.6920\n",
            "Epoch 5/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 5.6213 - val_loss: 2.6150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100\n",
            "70/70 [==============================] - 125s 2s/step - loss: 4.6959 - val_loss: 4.4971\n",
            "Epoch 7/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 4.1237 - val_loss: 5.3292\n",
            "Epoch 8/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 3.8636 - val_loss: 2.5748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 3.4367 - val_loss: 3.2483\n",
            "Epoch 10/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 3.1029 - val_loss: 4.0217\n",
            "Epoch 11/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 2.8167 - val_loss: 2.0345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 2.6081 - val_loss: 2.2065\n",
            "Epoch 13/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 2.3789 - val_loss: 3.0600\n",
            "Epoch 14/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.7319 - val_loss: 1.0639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.3798 - val_loss: 1.2818\n",
            "Epoch 16/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.1549 - val_loss: 0.9717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.1580 - val_loss: 0.8549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.0989 - val_loss: 1.1418\n",
            "Epoch 19/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.0495 - val_loss: 1.2784\n",
            "Epoch 20/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.1461 - val_loss: 1.2764\n",
            "Epoch 21/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.0883 - val_loss: 1.0730\n",
            "Epoch 22/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 1.0495 - val_loss: 1.4933\n",
            "Epoch 23/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8824 - val_loss: 0.8382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.9035 - val_loss: 0.8404\n",
            "Epoch 25/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8678 - val_loss: 0.8905\n",
            "Epoch 26/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8604 - val_loss: 0.9262\n",
            "Epoch 27/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8893 - val_loss: 0.8613\n",
            "Epoch 28/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.9113 - val_loss: 0.8566\n",
            "Epoch 29/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8415 - val_loss: 0.8329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8447 - val_loss: 0.8711\n",
            "Epoch 31/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8413 - val_loss: 0.8550\n",
            "Epoch 32/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8345 - val_loss: 0.8372\n",
            "Epoch 33/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8354 - val_loss: 0.8653\n",
            "Epoch 34/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8355 - val_loss: 0.8257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8418 - val_loss: 0.8355\n",
            "Epoch 36/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8462 - val_loss: 0.8234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8360 - val_loss: 0.8258\n",
            "Epoch 38/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8340 - val_loss: 0.8220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_and_return_conditional_losses, embedding_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, lin_tanh_layer_call_and_return_conditional_losses while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/casestudy/CommonReadability/readnet_tf/checkpoint/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8305 - val_loss: 0.8260\n",
            "Epoch 40/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8420 - val_loss: 0.8279\n",
            "Epoch 41/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8312 - val_loss: 0.8438\n",
            "Epoch 42/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8289 - val_loss: 0.8380\n",
            "Epoch 43/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8386 - val_loss: 0.8453\n",
            "Epoch 44/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8257 - val_loss: 0.8340\n",
            "Epoch 45/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8235 - val_loss: 0.8623\n",
            "Epoch 46/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8355 - val_loss: 0.8348\n",
            "Epoch 47/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8286 - val_loss: 0.8326\n",
            "Epoch 48/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8232 - val_loss: 0.8288\n",
            "Epoch 49/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8185 - val_loss: 0.8233\n",
            "Epoch 50/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8191 - val_loss: 0.8340\n",
            "Epoch 51/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8210 - val_loss: 0.8320\n",
            "Epoch 52/100\n",
            "70/70 [==============================] - 123s 2s/step - loss: 0.8232 - val_loss: 0.8268\n",
            "Epoch 53/100\n",
            "70/70 [==============================] - 124s 2s/step - loss: 0.8232 - val_loss: 0.8302\n"
          ]
        }
      ],
      "source": [
        "history = model.fit_generator(train_data_generator,\n",
        "                              steps_per_epoch=train_step_per_epoch,\n",
        "                              epochs = 100,\n",
        "                              validation_data = test_data_generator,\n",
        "                              validation_steps = test_step_per_epoch,\n",
        "                              callbacks = [callback_checkpoint,\n",
        "                                           callback_reducelr,\n",
        "                                           callback_earlystop\n",
        "                                           ]\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQnv6edybxal"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['loss'][1:])\n",
        "plt.plot(history.history['val_loss'][1:])\n",
        "plt.title('model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn7qeCrNjm9D",
        "outputId": "dcf6ef81-f611-4fd7-b635-d7a99ca323f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe0f1139290>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UUgUaB9alIh",
        "outputId": "1b085669-25af-43e8-ac04-1884009faa73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, lin_tanh_layer_call_fn while saving (showing 5 of 675). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Test/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: Test/assets\n"
          ]
        }
      ],
      "source": [
        "tf.keras.models.save_model(model,\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRM7Ig7gnSEF"
      },
      "outputs": [],
      "source": [
        "m = tf.keras.models.load_model(\"Test\",custom_objects={'rmse_loss': rmse_loss})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSJmtsmnlwt",
        "outputId": "722da54d-4e73-4217-d96d-7315f0f79ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train RMSE 0.8216878771781921\n",
            "Test RMSE 0.8258156776428223\n"
          ]
        }
      ],
      "source": [
        "print(\"Train RMSE\",m.evaluate_generator(train_data_generator))\n",
        "print(\"Test RMSE\",m.evaluate_generator(test_data_generator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKUXVdSYK2p7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ReadNet_TF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
